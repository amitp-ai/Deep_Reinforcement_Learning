'''
Reinforcement learning has long be thought to be an important tool in achieving human level Artificial Intelligence. While we are still far away from anything remotely like human level AI, the advent of deep learning has significantly imporved the performance of traditional reinforcement learning algorithms. In this article, we will look at my implementation for my first project (using Unity ML) in the Udacity Deep Reinforcement Learning Nanodegree program. 

The objective of the project is to collect as many yellow bananas while avoiding purple bananas. A reward of +1 is achieved for picking each yellow banana while a reward of -1 is achieved for picking a purple banana. The agent's observation space is 37 dimensional (including the agens velocity, image ray from the object, etc) and the agent's action space is 4 dimensional (forward, backward, turn left, and turn right). Before diving into the technical details, let us briefly cover the basics of Reinforcement learning.

In broad terms, machine learning can be divided into three categories: supervised learning, unsupervised learning, reinforcement learning. Supervised learning is when training a model directly from the ground truth and the model gets immediate feedback as to whether its prediction is correct or not. Unsupervised learning is training the model without having any knowledge for what is the correct answer. Reinforcement learning is training an agent to perform a particula behavior, but the feedback for whether the agents behavior is correct or not is received after many time steps. It is basically a method of sequential decision making where the model of the environment is unknown and the agent has to learn. If the environment's model is known, the it is known as a planning problem. There are many different planning methods such as Dynamic Programming, search methods, etc. Strictly speaking, regardless of whether the environment's model is known or not, it is all under the domain of reinforcement learning. However, in common parlance, reinforcement learning is typically used for situations where the environment's model is not known.

The basic framework for a reinforcement learning problem is defined using a Markvov Decision Process (MDP). In an MDP, and agent interacts with the environment by taking actions, and in return, the agent gets rewards and the next state information from the environment. And the process continues. The goal of the agent is to maximize it's rewards. Basically the goal is to find the policy that tells what actions to take form each state that will result in the maximum rewards. It is called Markvov because the state incorporates all the information necessary about the past (i.e. history) such that the future is independent of the past givent the present state. The probelm however is that in many real world situations, the enviromnemt is only partially observable i.e. we end up with a Partially Observable Markov Decision Process (POMDP). And when the environment is only partially observable, the Markov propety no longer holds as the future is dependent on the past, even given the present state. To convert it into a MDP, the state can be augment such that the current state is a combination of states from a few times steps in the past. But this doesn't always work as it is not clear howm many time steps in the past to look at. Another way is to use some sort of a recurrent models (e.g RNNs) that combines all the historical information (using some learned mathematical relationship) to turn partial observations into full state information.

Once given an MDP, the goal of an RL algorithm is taken the optimum actions to get the highest possible rewards. In order to solve an MDP, for each state, we need to determine what is value of it. And by definition, the value(s) of a state is the expected total reward (with the appropriate discounting factor) obtained by following the policy pi from that state onwards. The q-value(s,a) (state-action value function) is the expected total reward (with the appropriate discounting factor) obtained by taking action a from state s and then following the policy pi from that state onwards. There are two commonly used RL algortihms to solve MDPs: Monte-Carlo and Temporal Difference learning methods. 

Using Monte-Carlo learning, we start off with a random policy from a starting state s and taking action a and then following the policy pi. Then upon reaching the terminal state, the q-value of state s and ation a is updated using the following equation:

qpi(s,a) = qpi(s,a) + alpha(Gt-qpi(s,a))

This same process is done for all the states encountered along the way before reaching the terminal states. Then the policy is updated such that for each state that has been visited in the previous episode, we pick the action with the highest q-value. But given the q-values for the policy pi are not accurate as they're only an estimate, this will be a greedy policy that can be suboptimal. To address it, we use an e-greedy policy where with a probability of e, we randomly pick an action from a state s and with probability 1-e we follow the greedy policy. This helps balance exploration-exploitation tradeoff that is so important in reinforcement learning. The process of iteratively updating the e-greedy policy and the state-action values of all the states encountered before the episode terminates, eventually leads to state-action values (and thus policy) convergence to the optimal policy. One downside the the Monte-carlo method is that is only works for episodeic tasks (i.e. tasks that temrinate) also it takes longer time to learn than other algorithms such as TD learning.


Temporal difference learning is another type of Reinforcement learning algorithm that combines the best from Monte-Carlo learning and Dynamic programming. Like dynamic programming, we update the state-action values after one step instead of aiting till the episode termination. This allows for the algorithm to converge faster and be computationally efficient. Moreover, unlike DP but like MC, for each state, TD only takes/samples a single action. That is unlike DP, we don't do full action sweep at each step. This further makes it computationally efficient. There are a couple different variants of the TD learning algorithm: sarsa and q-learning. And thhey differ in how their update equations are calulated. The actiona-values update equation for sarsa is given by:

qpi(s,a) = qpi(s,a) + alpha(qpi(s',a') - qpi(s,a)) where a' is the action taken accoridng to policy pi from state s'

And the update equation for q-learning is:
qpi(s,a) = qpi(s,a) + alpha(maxa{qpi(s',a)} - qpi(s,a))

Once the state-action values are updated, the policy is intrun updated in the same was as with MC method above i.e. using e-greedy policy.


sarsa is an online learning algorithm because for the TD target (i.e. qpi(s',a')) the action a' is chosen based upon the policy we are trying to learn/improve i.e. pi. Whereas q-learning is an off-policy algorithm becasue the the TD target  (i.e. maxa{qpi(s',a)}) is chosen based upon the greedy policy, which is not same as the e-greedy policy we are trying to learn. And there are pros and cons to both approaches.

The above equations, however only work for tabular world cases where the state space is finite. In continuous environments, discretizing the statespace and quicly run into the curse of dimensionality. To address it, we instead use a function approximator to model the q-values. Using function approximator, we update the weights of the function approximator and so the update equation for q-learning becomes:

w = qpi_hat(s,a,w) + alpha(maxa{qpi(s',a,w)} - qpi(s,a,w))*grad(qpi(s,a,w), wrt w)

And the policy update is same as above where for each visited state, with probability e we select a random action and with probability 1-e we select an action with the maximum q-value (i.e. greedy policy). For linear function approximators, this approach works well in practice. That is the learning algorith doesn't oscillate and instead converges to the optimal policy. However, for nonlinear function approximators like neural networks, the above approach can run in to instabilities. To help improve convergence two modicifations can be made and the resulting algorithm is known as Deep Q-Network (DQN).


1. Fixed Q Target: In the above approach, the TD target is also dependent on the network parameter w that we are trying to learn/update which can lead to instabilities. To address it, a separate network with identical architecture but different weights is use. And the weights of this separate target ntework are updated every 100 or steps to be equal to the local network (i.e. the nwtwork that is continuously being learned).

2. The second modification is experience replay. Updating the weights as new states are being visites is fraight is problems. One is that we don't make use of past experiences. And experience is only used once and then discared. An even worse problem is that there is inherent correlation in the states being visited that needs to be broken otherwise the agen will not learng well. Both of these issues are address using experience replay where we have a memory buffer where all the expriences (i.e. state, action, reward, and next state) tuples are stored. And to break correlation, at each learning step, we randomly sample experiences form this buffer to break correlation. This also helps us learn from the same experience multiple times. This is especially useful when encountering some rare experiences.

Double DQN




'''
