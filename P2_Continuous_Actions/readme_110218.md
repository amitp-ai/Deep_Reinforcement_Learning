
Value based reinforcement learning algorithms such as DQN have shown great performance in many domains. However, they are still limited to discrete action space environments and for deterministic policies (as they are essentially based upon a deterministic greedy policy as epsilon only selects a random action).

Moreover, with value based methods, we first compute the value-function for each state, and use that to determine the best policy. This is an indirect way of finding the optimal policy.

Using a policy based method, we directly find the policy that yields the most rewards. Policy gradient is one of the more efficient policy based learning algorithms where we directly compute the gradient of the expected reward with respect to the policy parameters. In addition to being direct, it works well with continuous actions as well as stochastic policies. other policy based methods include CEM.

One downside of the basic policy gradient algorithm is that the gradient is dependent on the Monte-Carlo estimate of the expected reward from state s -- which while it has low bias, has high variance. By subtracting out a baseline from the Monte-Carlo estimate, we can reduce the variance. The variance can be further reduce by using a TD estimate of the total rewards instead of a Monte-Carlo estimate. While TD estimate has higher bias, it's much lower variance is worth the tradeoff.

Unsurprisingly, as with value based RL algorithms, deep learning has significantly improved the performance achieved using policy gradient. One such algortih is the actor-critic algorithm. Where the policy (actor) is representaed by a deep neural network, and the TD estimate (critic) is represented by another neural network. The output of the actor network represents the mean and covariance of continuous actions selected using a Gaussian distribution.

If the problem domain is deterministic, then we can get a significant improvement in training speed using a deterministic network. One such algortihm is the deep deterministic policy gradient. It is basically a deterministic version of actor-critic. It doesn't subtract out a baseline (i.e. expected value) from the TD estimate because it is a deterministic network.

In the second project of the Udacity's Deep Reinforcement Learning Nanodegree, the goal is to train an acrobat arm that has two joints so that it tracks a baloon. As the baloon moves, the two joints at adjusted to track the baloon. So this is a classical robotics project, and using model-free reinforcement learning, the agent will learn the optimal policy. In particular, the method used is the deep deterministic policy gradient method (DDPG).

There are 4 separate neural networks used. One to learn the policy, another to learn the value function, another for the target value fucntion, and lastly for the target action in the target value function network. We use a separate target network instead of the local network so as to prevent any instabilities in learning when the TD target is dependedn on the local network. The weights in the target network are updated very slowly, like 0.1% towards the local network at every time step. Slowly changing the target network does slow down the learning rate, but it helps with the learning algorithm's stability.


Also try to do the crawler project (there is a bug in the project that it gave error to other students)

Goal in RL is to maximize U<sub>&theta;</sub>, which is defined as follows:

<img src="https://latex.codecogs.com/png.latex?\fn_cm&space;U_{\theta}&space;=&space;E_{\tau&space;\,&space;\sim&space;\,&space;P_{\theta}(\tau)}&space;\left&space;[&space;\sum_{t=0}^{T}r(s_{t},a_{t})&space;\right&space;],&space;whereby&space;\,&space;\tau&space;=&space;\left&space;\{&space;s_{0},&space;a_{0},&space;s_{1},&space;a_{1},&space;...,&space;s_{T},&space;a_{T}&space;\right&space;\}&space;\:&space;...&space;\,&space;Equation&space;\,&space;1a" title="U_{\theta} = E_{\tau \, \sim \, P_{\theta}(\tau)} \left [ \sum_{t=0}^{T}r(s_{t},a_{t}) \right ], whereby \, \tau = \left \{ s_{0}, a_{0}, s_{1}, a_{1}, ..., s_{T}, a_{T} \right \} \: ... \, Equation \, 1a" />

After doing some Math, it can be shown that Utheta is equal to the expected value of Q(s0,a0). And if the initial state distribution is uniform, then it means the goal in RL is to find a policy which maximizes the q-value of all possible state-action pairs.

<img src="https://latex.codecogs.com/png.latex?\fn_cm&space;U_{\theta}&space;=&space;E_{s_{0},&space;a_{0}&space;\,&space;\sim&space;\,&space;P(s_{0},&space;a_{0})}&space;\left&space;[&space;Q_{P_{\theta}}(s_{0},&space;a_{0})&space;\right&space;]&space;\:&space;...&space;\,&space;Equation&space;\,&space;1b" title="U_{\theta} = E_{s_{0}, a_{0} \, \sim \, P(s_{0}, a_{0})} \left [ Q_{P_{\theta}}(s_{0}, a_{0}) \right ] \: ... \, Equation \, 1b" />

Using the definition of expectation, the above equation 1a can be re-written as:

<img src="https://latex.codecogs.com/png.latex?\fn_cm&space;U_{\theta}&space;=&space;\sum_{\tau}&space;\left&space;(P_{\theta}(\tau)&space;\left(\sum_{t=0}^{T}r(s_{t},a_{t})\right)&space;\right&space;)&space;...&space;\:&space;Equation&space;\:&space;2" title="U_{\theta} = \sum_{\tau} \left (P_{\theta}(\tau) \left(\sum_{t=0}^{T}r(s_{t},a_{t})\right) \right ) ... \: Equation \: 2" />

Using Policy gradient method, we can maximize U<sub>&theta;</sub> by first computing its gradeint with respect to &theta;, which readily be derived to be:

<img src="https://latex.codecogs.com/png.latex?\fn_cm&space;\nabla_{\theta}U_{\theta}&space;=&space;E_{\tau&space;\,&space;\sim&space;\,&space;P_{\theta}(\tau)}&space;\left[&space;\sum_{t=0}^{T}\left(\nabla_{\theta}log(P_{\theta}(a_{t}|s_{t}))&space;\left(&space;\sum_{t=0}^{T}r(s_{t},a_{t})&space;\right)&space;\right)&space;\right]&space;\:&space;...&space;\,&space;Equation&space;\,&space;3" title="\nabla_{\theta}U_{\theta} = E_{\tau \, \sim \, P_{\theta}(\tau)} \left[ \sum_{t=0}^{T}\left(\nabla_{\theta}log(P_{\theta}(a_{t}|s_{t})) \left( \sum_{t=0}^{T}r(s_{t},a_{t}) \right) \right) \right] \: ... \, Equation \, 3" />

One method to change &theta; to improve the expected total reward is randomly add noise to the current &theta; and it results in better expected reward, then we keep it otherwise we ignore and we keep repeating this process. This method is called the random shooting method. There are other more sophisticated methods in the same vein such as the Criss Entropy Method. All these methods fall under the domain of stochastic optimization algorithms. However, while these methods ase very simple to implement, then are not efficient and don't scale well. A more efficient approach is to change &theta; in the direction of maximum ascent using Stochastic Gradient Ascent as follows:

<img src="https://latex.codecogs.com/png.latex?\fn_cm&space;\theta&space;\leftarrow&space;\theta&space;&plus;&space;\alpha\nabla_{\theta}U_{\theta}&space;\;&space;...&space;\;&space;Equation&space;\,&space;4" title="\theta \leftarrow \theta + \alpha\nabla_{\theta}U_{\theta} \; ... \; Equation \, 4" />

A basic policy gradient algorithm making use of the above gradient is known as the Reinforce algorithm, and here is how it works, starting with a random vector &theta;:
Repeat the following until convergence:
1. Use the policy P<sub>&theta;</sub>(a<sub>t</sub>|s<sub>t</sub>) to collect m trajectories {&tau;<sup>1</sup>, &tau;<sup>2</sup>, ..., &tau;<sup>m</sup>}, where each trajectory is as defined above.
2. Use these trajectories to compute the Monte-Carlo estimate of the gradient as follows:

<img src="https://latex.codecogs.com/png.latex?\fn_cm&space;\nabla_{\theta}U_{\theta}&space;\approx&space;\hat{g}&space;=&space;\frac{1}{m}&space;\sum_{i=1}^{m}&space;\left(&space;\sum_{t=0}^{T}&space;\left(&space;\nabla_{\theta}log(P_{\theta}(a_{t}|s_{t}))&space;\left(&space;\sum_{t=0}^{T}r(s_{t},a_{t})&space;\right)&space;\right)&space;\right)&space;\:&space;...&space;\,&space;Equation&space;\,&space;5" title="\nabla_{\theta}U_{\theta} \approx \hat{g} = \frac{1}{m} \sum_{i=1}^{m} \left( \sum_{t=0}^{T} \left( \nabla_{\theta}log(P_{\theta}(a_{t}|s_{t})) \left( \sum_{t=0}^{T}r(s_{t},a_{t}) \right) \right) \right) \: ... \, Equation \, 5" />

Note that the reason why the above estimate is valid is because the trajectories are generated by following the policy being learned, i.e. P<sub>&theta;</sub>(&tau;) -- i.e. it is an on-policy algorithm. Another way to say it is that we sample each of the trajectories in {&tau;<sup>1</sup>, &tau;<sup>2</sup>, ..., &tau;<sup>m</sup>} from the probability distribution P<sub>&theta;</sub>(&tau;).

3. Update the weights/parameters of the policy network using the above estimate of the gradient:

<img src="https://latex.codecogs.com/png.latex?\fn_cm&space;\theta&space;\leftarrow&space;\theta&space;&plus;&space;\alpha\hat{g}&space;\;&space;...&space;\;&space;Equation&space;\,&space;6" title="\theta \leftarrow \theta + \alpha\hat{g} \; ... \; Equation \, 6" />

The intuition behind the reinforce algorithm is that if the total reward is positive, then all the actions taken in that trajectory are reinforced whereas if the total rewards is negative, then all the actions taken in the trajectory are inhibited. This is not great as will be discussed below.

In order to speed up learning, typically m is set to 1. While better than stochastic optimization methods, the Reinforce algorithm suffers from a few drawbacks:
1. The gradient estimate is pretty noisy, especially for the case m=1, because a single trajectory maynot be representative of the policy.
2. There is no clear credit assignment. A trajectory may contain many good and bad actions, and whether those actions are reinforced depends only on the total reward achieved starting from the initial state.

By the definition of the gradient, nabla_theta points in the direction of maximum change in Utheta. However, the drawbacks of Reinforce algorithm are due to the fact that the Monte-Carlo estimate of nabla_theta (&gcirc;) has high variance. If we can reduce its variance, then out estimate of gradient &gcirc; will be closer to the true gradient nable_theta. This is precisely what we do with actor-critic algorithm.

While the Monte-Carlo estimate of the gradient &gcirc; is unbiased, it exhibits high variance. There are a few ways of reducing variance without introducing any bias.

1. Using Causality: &gcirc; updates all the actions in a trajectory based upon total rewards and not the rewards to go. That is to say, future actions affect past rewards, which is not possible in our causal Universe. So we can make the gradient estimate more realistic by using rewards to go as shown in the below equation.

<img src="https://latex.codecogs.com/png.latex?\fn_cm&space;\nabla_{\theta}U_{\theta}&space;\approx&space;\hat{g}&space;=&space;\frac{1}{m}&space;\sum_{i=1}^{m}&space;\left(&space;\sum_{t=0}^{T}&space;\left(&space;\nabla_{\theta}log(P_{\theta}(a_{t}|s_{t}))&space;\left(&space;\sum_{t^{'}=t}^{T}r(s_{t^{'}},a_{t^{'}})&space;\right)&space;\right)&space;\right)&space;\:&space;...&space;\,&space;Equation&space;\,&space;7" title="\nabla_{\theta}U_{\theta} \approx \hat{g} = \frac{1}{m} \sum_{i=1}^{m} \left( \sum_{t=0}^{T} \left( \nabla_{\theta}log(P_{\theta}(a_{t}|s_{t})) \left( \sum_{t^{'}=t}^{T}r(s_{t^{'}},a_{t^{'}}) \right) \right) \right) \: ... \, Equation \, 7" />

Note that using the rewards to go instead of the total rewards is still an unbiased estimate of &nabla;<sub>&theta;</sub>, because causality is handled in the expectation using P<sub>&theta;</sub>(&tau;).

An important aside to note is that the rewards to go is really an estimate of the the q-value of (s<sub>t</sub>, a<sub>t</sub>). This is because the q-value is defined as follows:

<img src="https://latex.codecogs.com/png.latex?\fn_cm&space;Q_{P_{\theta}}(s_{t},&space;a_{t})&space;=&space;r(s_t,&space;a_t)&space;&plus;&space;E_{\tau&space;\sim&space;P_{\theta}(\tau|s_t,&space;a_t)}\left&space;[\sum&space;_{t^{'}=t&plus;1}^T&space;r(s_{t^{'}},&space;a_{t^{'}})&space;\mid&space;s_t,&space;a_t&space;\right&space;]&space;\:&space;...&space;\,&space;Equation&space;\,&space;8" title="Q_{P_{\theta}}(s_{t}, a_{t}) = r(s_t, a_t) + E_{\tau \sim P_{\theta}(\tau|s_t, a_t)}\left [\sum _{t^{'}=t+1}^T r(s_{t^{'}}, a_{t^{'}}) \mid s_t, a_t \right ] \: ... \, Equation \, 8" />


And so, if the trajectory &tau; is sampled from P<sub>&theta;</sub>(&tau;), then the Monte-Carlo estimate of Q<sub>P<sub>&theta;</sub></sub>(s<sub>t</sub>, a<sub>t</sub>) is just:

<img src="https://latex.codecogs.com/png.latex?\fn_cm&space;Q_{P_{\theta}}(s_{t},&space;a_{t})&space;\approx&space;\hat{Q}_{P_{\theta}}(s_{t},&space;a_{t})&space;=&space;\sum_{t^{'}=t}^T&space;r(s_{t^{'}},&space;a_{t^{'}})&space;\:&space;...&space;\,&space;Equation&space;\,&space;9" title="Q_{P_{\theta}}(s_{t}, a_{t}) \approx \hat{Q}_{P_{\theta}}(s_{t}, a_{t}) = \sum_{t^{'}=t}^T r(s_{t^{'}}, a_{t^{'}}) \: ... \, Equation \, 9" />

As a result, Equation 7 can be re-written as:

<img src="https://latex.codecogs.com/png.latex?\fn_cm&space;\nabla_{\theta}U_{\theta}&space;\approx&space;\hat{g}&space;=&space;\frac{1}{m}&space;\sum_{i=1}^{m}&space;\left(&space;\sum_{t=0}^{T}&space;\left(&space;\nabla_{\theta}log(P_{\theta}(a_{t}|s_{t}))&space;\,&space;\hat{Q}_{P_\theta}(s_t,&space;a_t)&space;\right)&space;\right)&space;\:&space;...&space;\,&space;Equation&space;\,&space;10" title="\nabla_{\theta}U_{\theta} \approx \hat{g} = \frac{1}{m} \sum_{i=1}^{m} \left( \sum_{t=0}^{T} \left( \nabla_{\theta}log(P_{\theta}(a_{t}|s_{t})) \, \hat{Q}_{P_\theta}(s_t, a_t) \right) \right) \: ... \, Equation \, 10" />

If Qhat<sub>P<sub>&theta;</sub></sub>(s<sub>t</sub>, a<sub>t</sub>) is modelled using a neural network (parameterized by w) such that:

<img src="https://latex.codecogs.com/png.latex?\fn_cm&space;\hat{Q}_{P_{\theta}}(s_{t},&space;a_{t})&space;=&space;\hat{Q}_{P_{\theta}}(s_{t},&space;a_{t},&space;w)&space;\:&space;...&space;\,&space;Equation&space;\,&space;11" title="\hat{Q}_{P_{\theta}}(s_{t}, a_{t}) = \hat{Q}_{P_{\theta}}(s_{t}, a_{t}, w) \: ... \, Equation \, 11" />

Then Equation 10 can be re-written as:

<img src="https://latex.codecogs.com/png.latex?\fn_cm&space;\nabla_{\theta}U_{\theta}&space;\approx&space;\hat{g}&space;=&space;\frac{1}{m}&space;\sum_{i=1}^{m}&space;\left(&space;\sum_{t=0}^{T}&space;\left(&space;\nabla_{\theta}log(P_{\theta}(a_{t}|s_{t}))&space;\,&space;\hat{Q}_{P_\theta}(s_t,&space;a_t,&space;w)&space;\right)&space;\right)&space;\:&space;...&space;\,&space;Equation&space;\,&space;12" title="\nabla_{\theta}U_{\theta} \approx \hat{g} = \frac{1}{m} \sum_{i=1}^{m} \left( \sum_{t=0}^{T} \left( \nabla_{\theta}log(P_{\theta}(a_{t}|s_{t})) \, \hat{Q}_{P_\theta}(s_t, a_t, w) \right) \right) \: ... \, Equation \, 12" />

Whereby, P<sub>&theta;</sub>(a<sub>t</sub> | s<sub>t</sub>) is the actor network that is parameterized by &theta; and Qhat<sub>P<sub>&theta;</sub></sub>(s<sub>t</sub>, a<sub>t</sub>, w) is the critic network that is parameterized by w. This is essentially what is known as an actor-critic algorithm.

The actor network is updated using Equation 6, and the critic network is typically updated using TD-learning using the following update equation:


