Value based reinforcement learning algorithms such as DQN have shown great performance in many domains. However, they are still limited to discrete action space environments and for deterministic policies (as they are essentially based upon a deterministic greedy policy as epsilon only selects a random action).

Moreover, with value based methods, we first compute the value-function for each state, and use that to determine the best policy. This is an indirect way of finding the optimal policy.

Using a policy based method, we directly find the policy that yields the most rewards. Policy gradient is one of the more efficient policy based learning algorithms where we directly compute the gradient of the expected reward with respect to the policy parameters. In addition to being direct, it works well with continuous actions as well as stochastic policies. other policy based methods include CEM.

One downside of the basic policy gradient algorithm is that the gradient is dependent on the Monte-Carlo estimate of the expected reward from state s -- which while it has low bias, has high variance. By subtracting out a baseline from the Monte-Carlo estimate, we can reduce the variance. The variance can be further reduce by using a TD estimate of the total rewards instead of a Monte-Carlo estimate. While TD estimate has higher bias, it's much lower variance is worth the tradeoff.

Unsurprisingly, as with value based RL algorithms, deep learning has significantly improved the performance achieved using policy gradient. One such algortih is the actor-critic algorithm. Where the policy (actor) is representaed by a deep neural network, and the TD estimate (critic) is represented by another neural network. The output of the actor network represents the mean and covariance of continuous actions selected using a Gaussian distribution.

If the problem domain is deterministic, then we can get a significant improvement in training speed using a deterministic network. One such algortihm is the deep deterministic policy gradient. It is basically a deterministic version of actor-critic. It doesn't subtract out a baseline (i.e. expected value) from the TD estimate because it is a deterministic network.

In the second project of the Udacity's Deep Reinforcement Learning Nanodegree, the goal is to train an acrobat arm that has two joints so that it tracks a baloon. As the baloon moves, the two joints at adjusted to track the baloon. So this is a classical robotics project, and using model-free reinforcement learning, the agent will learn the optimal policy. In particular, the method used is the deep deterministic policy gradient method (DDPG).

There are 4 separate neural networks used. One to learn the policy, another to learn the value function, another for the target value fucntion, and lastly for the target action in the target value function network. We use a separate target network instead of the local network so as to prevent any instabilities in learning when the TD target is dependedn on the local network. The weights in the target network are updated very slowly, like 0.1% towards the local network at every time step. Slowly changing the target network does slow down the learning rate, but it helps with the learning algorithm's stability.


Also try to do the crawler project (there is a bug in the project that it gave error to other students)

Goal in RL is to maximize U<sub>&theta;</sub>, which is defined as follows:

<img src="https://latex.codecogs.com/png.latex?\fn_cm&space;U_{\theta}&space;=&space;E_{\tau&space;\,&space;\sim&space;\,&space;P_{\theta}(\tau)}&space;\left&space;[&space;\sum_{t=0}^{T}r(s_{t},a_{t})&space;\right&space;],&space;whereby&space;\,&space;\tau&space;=&space;\left&space;\{&space;s_{0},&space;a_{0},&space;s_{1},&space;a_{1},&space;...,&space;s_{T},&space;a_{T}&space;\right&space;\}&space;\:&space;...&space;\,&space;Equation&space;\,&space;1" title="U_{\theta} = E_{\tau \, \sim \, P_{\theta}(\tau)} \left [ \sum_{t=0}^{T}r(s_{t},a_{t}) \right ], whereby \, \tau = \left \{ s_{0}, a_{0}, s_{1}, a_{1}, ..., s_{T}, a_{T} \right \} \: ... \, Equation \, 1" />

Using the definition of expectation, the above equation can be re-written as:

<img src="https://latex.codecogs.com/png.latex?\fn_cm&space;U_{\theta}&space;=&space;\sum_{\tau}&space;\left&space;(P_{\theta}(\tau)&space;\left(\sum_{t=0}^{T}r(s_{t},a_{t})\right)&space;\right&space;)&space;...&space;\:&space;Equation&space;\:&space;2" title="U_{\theta} = \sum_{\tau} \left (P_{\theta}(\tau) \left(\sum_{t=0}^{T}r(s_{t},a_{t})\right) \right ) ... \: Equation \: 2" />

Using Policy gradient method, we can maximize U<sub>&theta;</sub> by first computing its gradeint with respect to &theta;, which readily be derived to be:

<img src="https://latex.codecogs.com/png.latex?\fn_cm&space;\nabla_{\theta}U_{\theta}&space;=&space;E_{\tau&space;\,&space;\sim&space;\,&space;P_{\theta}(\tau)}&space;\left[&space;\sum_{t=0}^{T}\left(\nabla_{\theta}log(P_{\theta}(a_{t}|s_{t}))&space;\left(&space;\sum_{t=0}^{T}r(s_{t},a_{t})&space;\right)&space;\right)&space;\right]&space;\:&space;...&space;\,&space;Equation&space;\,&space;3" title="\nabla_{\theta}U_{\theta} = E_{\tau \, \sim \, P_{\theta}(\tau)} \left[ \sum_{t=0}^{T}\left(\nabla_{\theta}log(P_{\theta}(a_{t}|s_{t})) \left( \sum_{t=0}^{T}r(s_{t},a_{t}) \right) \right) \right] \: ... \, Equation \, 3" />

One method to change &theta; to improve the expected total reward is randomly add noise to the current &theta; and it results in better expected reward, then we keep it otherwise we ignore and we keep repeating this process. This method is called the random shooting method. There are other more sophisticated methods in the same vein such as the Criss Entropy Method. However, while these methods ase very simple to implement, then are not efficient and don't scale well. A more efficient approach is to change &theta; in the direction of maximum ascent using Stochastic Gradient Ascent as follows:

<img src="https://latex.codecogs.com/png.latex?\fn_cm&space;\theta&space;\leftarrow&space;\theta&space;&plus;&space;\alpha\nabla_{\theta}U_{\theta}&space;\;&space;...&space;\;&space;Equation&space;\,&space;4" title="\theta \leftarrow \theta + \alpha\nabla_{\theta}U_{\theta} \; ... \; Equation \, 4" />

A basic policy gradient algorithm making use of the above gradient is known as the Reinforce algorithm, and here is how it works, starting with a random vector &theta;:
Repeat the following until convergence:
1. Use the policy P<sub>&theta;</sub>(a<sub>t</sub>|s<sub>t</sub>) to collect m trajectories {&tau;<sup>1</sup>, &tau;<sup>2</sup>, ..., &tau;<sup>m</sup>}, where each trajectory is as defined above.
2. Use these trajectories to compute the Monte-Carlo estimate of the gradient as follows:

<img src="https://latex.codecogs.com/png.latex?\fn_cm&space;\nabla_{\theta}U_{\theta}&space;\approx&space;\hat{g}&space;=&space;\frac{1}{m}&space;\sum_{i=1}^{m}&space;\left(&space;\sum_{t=0}^{T}&space;\left(&space;\nabla_{\theta}log(P_{\theta}(a_{t}|s_{t}))&space;\left(&space;\sum_{t=0}^{T}r(s_{t},a_{t})&space;\right)&space;\right)&space;\right)&space;\:&space;...&space;\,&space;Equation&space;\,&space;5" title="\nabla_{\theta}U_{\theta} \approx \hat{g} = \frac{1}{m} \sum_{i=1}^{m} \left( \sum_{t=0}^{T} \left( \nabla_{\theta}log(P_{\theta}(a_{t}|s_{t})) \left( \sum_{t=0}^{T}r(s_{t},a_{t}) \right) \right) \right) \: ... \, Equation \, 5" />

3. Update the weights/parameters of the policy network using the above estimate of the gradient:

<img src="https://latex.codecogs.com/png.latex?\fn_cm&space;\theta&space;\leftarrow&space;\theta&space;&plus;&space;\alpha\hat{g}&space;\;&space;...&space;\;&space;Equation&space;\,&space;6" title="\theta \leftarrow \theta + \alpha\hat{g} \; ... \; Equation \, 6" />

