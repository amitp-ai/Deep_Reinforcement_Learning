Value based reinforcement learning algorithms such as DQN have shown great performance in many domains. However, they are still limited to discrete action space environments and for deterministic policies (as they are essentially based upon a deterministic greedy policy as epsilon only selects a random action).

Moreover, with value based methods, we first compute the value-function for each state, and use that to determine the best policy. This is an indirect way of finding the optimal policy.

Using a policy based method, we directly find the policy that yields the most rewards. Policy gradient is one of the more efficient policy based learning algorithms where we directly compute the gradient of the expected reward with respect to the policy parameters. In addition to being direct, it works well with continuous actions as well as stochastic policies. other policy based methods include CEM.

One downside of the basic policy gradient algorithm is that the gradient is dependent on the Monte-Carlo estimate of the expected reward from state s -- which while it has low bias, has high variance. By subtracting out a baseline from the Monte-Carlo estimate, we can reduce the variance. The variance can be further reduce by using a TD estimate of the total rewards instead of a Monte-Carlo estimate. While TD estimate has higher bias, it's much lower variance is worth the tradeoff.

Unsurprisingly, as with value based RL algorithms, deep learning has significantly improved the performance achieved using policy gradient. One such algortih is the actor-critic algorithm. Where the policy (actor) is representaed by a deep neural network, and the TD estimate (critic) is represented by another neural network. The output of the actor network represents the mean and covariance of continuous actions selected using a Gaussian distribution.

If the problem domain is deterministic, then we can get a significant improvement in training speed using a deterministic network. One such algortihm is the deep deterministic policy gradient. It is basically a deterministic version of actor-critic. It doesn't subtract out a baseline (i.e. expected value) from the TD estimate because it is a deterministic network.

In the second project of the Udacity's Deep Reinforcement Learning Nanodegree, the goal is to train an acrobat arm that has two joints so that it tracks a baloon. As the baloon moves, the two joints at adjusted to track the baloon. So this is a classical robotics project, and using model-free reinforcement learning, the agent will learn the optimal policy. In particular, the method used is the deep deterministic policy gradient method (DDPG).

There are 4 separate neural networks used. One to learn the policy, another to learn the value function, another for the target value fucntion, and lastly for the target action in the target value function network. We use a separate target network instead of the local network so as to prevent any instabilities in learning when the TD target is dependedn on the local network. The weights in the target network are updated very slowly, like 0.1% towards the local network at every time step. Slowly changing the target network does slow down the learning rate, but it helps with the learning algorithm's stability.


Also try to do the crawler project (there is a bug in the project that it gave error to other students)

Goal in RL is:

<img src="https://latex.codecogs.com/png.latex?\fn_cm&space;U_{\theta}&space;=&space;E_{\tau&space;\,&space;\sim&space;\,&space;p_{\theta}(\tau)}&space;\left&space;[&space;\sum_{t=0}^{T}r(s_{t},a_{t})&space;\right&space;]" title="U_{\theta} = E_{\tau \, \sim \, p_{\theta}(\tau)} \left [ \sum_{t=0}^{T}r(s_{t},a_{t}) \right ]" />

<img src="https://latex.codecogs.com/gif.latex?\fn_cm&space;whereby,\,&space;\tau&space;=&space;\left&space;\{&space;s_{0},&space;a_{0},&space;s_{1},&space;a_{1},&space;...,&space;s_{T},&space;a_{T}&space;\right&space;\}" title="whereby,\, \tau = \left \{ s_{0}, a_{0}, s_{1}, a_{1}, ..., s_{T}, a_{T} \right \}" />



